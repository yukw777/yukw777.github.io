@inproceedings{zhang-etal-2022-danli,
    title = "{DANLI}: Deliberative Agent for Following Natural Language Instructions",
    author = "Zhang, Yichi  and
      Yang, Jianing  and
      Pan, Jiayi  and
      Storks, Shane  and
      Devraj, Nikhil  and
      Ma, Ziqiao  and
      Yu, Keunwoo Peter  and
      Bao, Yuwei  and
      Chai, Joyce",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.83",
    doi = "10.18653/v1/2022.emnlp-main.83",
    pages = "1280--1298"
}
@inproceedings{bao-etal-2023-foundation,
    title = "Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?",
    author = "Bao, Yuwei  and
      Yu, Keunwoo  and
      Zhang, Yichi  and
      Storks, Shane  and
      Bar-Yossef, Itamar  and
      de la Iglesia, Alex  and
      Su, Megan  and
      Zheng, Xiao  and
      Chai, Joyce",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.824",
    doi = "10.18653/v1/2023.findings-emnlp.824",
    pages = "12325--12341",
    abstract = "Despite tremendous advances in AI, it remains a significant challenge to develop interactive task guidance systems that can offer situated, personalized guidance and assist humans in various tasks. These systems need to have a sophisticated understanding of the user as well as the environment, and make timely accurate decisions on when and what to say. To address this issue, we created a new multimodal benchmark dataset, Watch, Talk and Guide (WTaG) based on natural interaction between a human user and a human instructor. We further proposed two tasks: User and Environment Understanding, and Instructor Decision Making. We leveraged several foundation models to study to what extent these models can be quickly adapted to perceptually enabled task guidance. Our quantitative, qualitative, and human evaluation results show that these models can demonstrate fair performances in some cases with no task-specific training, but a fast and reliable adaptation remains a significant challenge. Our benchmark and baselines will provide a stepping stone for future work on situated task guidance.",
}
@inproceedings{storks-etal-2023-nlp,
    title = "{NLP} Reproducibility For All: Understanding Experiences of Beginners",
    author = "Storks, Shane  and
      Yu, Keunwoo Peter and
      Ma, Ziqiao  and
      Chai, Joyce",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.568",
    doi = "10.18653/v1/2023.acl-long.568",
    pages = "10199--10219"
}
@inproceedings{yu2024eliciting,
    title={Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties},
    author={Keunwoo Peter Yu and Zheyuan Zhang and Fengyuan Hu and Shane Storks and Joyce Chai},
    year={2024},
    booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
    url = "https://arxiv.org/abs/2311.17041"
}
